import pathway as pw
from pathway.xpacks.llm.document_store import DocumentStore
from pathway.xpacks.llm.servers import DocumentStoreServer
from pathway.stdlib.indexing import HybridIndexFactory, BruteForceKnnFactory, TantivyBM25Factory
from sentence_transformers import SentenceTransformer

# -----------------------------------------------------
# CONFIGURATION
# -----------------------------------------------------
VECTOR_STORE_PATH = "./vector_store"  # Folder created by your ingestion script
MODEL_NAME = "all-MiniLM-L6-v2"
HOST = "0.0.0.0"
PORT = 8000

# -----------------------------------------------------
# HELPER: Query Embedder
# -----------------------------------------------------
# Since we are not using OpenAI, we need this wrapper to 
# tell Pathway how to turn a USER QUERY into a vector 
# using the same model you used for the books.
class LocalEmbedder:
    def __init__(self, model_name):
        self.model = SentenceTransformer(model_name)

    def __call__(self, text):
        return self.model.encode(text).tolist()

# -----------------------------------------------------
# STEP 1: Read the pre-computed Vectors
# -----------------------------------------------------
# We read the JSON files generated by your previous code
documents = pw.io.fs.read(
    VECTOR_STORE_PATH,
    format="json",
    mode="static"
)

# -----------------------------------------------------
# STEP 2: Map Columns to Pathway Standards
# -----------------------------------------------------
# Your code produced: 'chunk_text', 'embedding', 'book_name'
# Pathway DocumentStore expects: 'text', 'vector'
processed_docs = documents.select(
    text=pw.this.chunk_text,    # Map chunk_text -> text
    vector=pw.this.embedding,   # Map embedding -> vector
    book_name=pw.this.book_name # Keep book_name for filtering
)

# -----------------------------------------------------
# STEP 3: Setup Hybrid Search (BM25 + KNN)
# -----------------------------------------------------

# A. The Query Embedder
# This runs live when a user searches to embed their question
query_embedder = LocalEmbedder(MODEL_NAME)

# B. KNN Index (Vector Search)
# Finds semantic matches. We pass the embedder here.
knn_index = BruteForceKnnFactory(
    embedder=query_embedder, 
    dimensions=384 # Dimension of all-MiniLM-L6-v2
)

# C. BM25 Index (Keyword Search)
# Finds exact word matches (names, years)
bm25_index = TantivyBM25Factory()

# D. Combine them
hybrid_factory = HybridIndexFactory([knn_index, bm25_index])

# -----------------------------------------------------
# STEP 4: Create the Document Store
# -----------------------------------------------------
vector_store = DocumentStore(
    processed_docs,
    retriever_factory=hybrid_factory
)

# -----------------------------------------------------
# STEP 5: Start the Server
# -----------------------------------------------------
server = DocumentStoreServer(
    host=HOST,
    port=PORT,
    document_store=vector_store
)

print(f"ðŸš€ Hybrid Retrieval Server running at http://{HOST}:{PORT}")
print(f"ðŸ§  Using local model: {MODEL_NAME}")
print("âœ… Ready to accept queries!")

server.run()